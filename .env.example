# Inference Service Configuration
INFERENCE_PORT=8888
# Path to the model directory (mounted to /app/models) or HuggingFace ID
INFERENCE_MODEL=./models/lotus-12B

# Quantization Level: 4bit, 8bit, 16bit, none
QUANTIZATION=4bit

# Use SafeTensors: TRUE, FALSE
USE_SAFETENSORS=FALSE

# Device Map: auto, cuda:0, etc.
DEVICE_MAP=auto

# Gateway Service Configuration
GATEWAY_PORT=9009
INFERENCE_MODEL_NAME=Lotus-12B
INFERENCE_AUTHOR=Haru
INFERENCE_DESCRIPTION=lotus-12b
